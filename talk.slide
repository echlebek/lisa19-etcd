Creating A Distributed Round Robin Scheduler with Etcd
A developer learns some things about distributed systems and reliability
Tags: etcd, roundrobin, sensu, sensu-go, scheduler, distributed, raft

Eric Chlebek
Software Developer, Sensu
eric@sensu.io
http://sensu.io
@EricChlebek

* About Me

.link https://github.com/echlebek https://github.com/echlebek
.link https://lisa19-etcd.herokuapp.com/talk.slide#1 https://lisa19-etcd.herokuapp.com/talk.slide#1

- Works @ Sensu on the Sensu Go monitoring project.
- Experience in HPC, Bioinformatics, Ad-tech, Systems Monitoring.

.image ./content/me.jpg _ 300

* Introduction

* Introduction

.image ./content/ytho.jpg _ 500

* Introduction

.image ./content/sensu-logo-horizontal.png _ 350

- Sensu is a monitoring framework for heterogeneous systems.
- For the purposes of this talk, Sensu is a scheduler for executing host-based checks on subscriber nodes.
- Round robin scheduling is one of the key features of Sensu's scheduler.

* Introduction

.image ./content/sensu-logo-horizontal.png _ 200

- By default, all systems execute their subscribed checks at every scheduling interval.
- Some use cases are better suited a round-robin mode of scheduling (load-balanced websites, network switches)
- Classic Sensu relied on RabbitMQ for round-robin scheduling, and Redis for state.

* Introduction

.image ./content/RabbitMQ-logo.svg _ 400

- Classic Sensu uses RabbitMQ as a broker to distribute tasks to clients.
- Single leader responsible for sending tasks to the queue.
- Randomized consumption of tasks by clients.
- In clustered/HA scenario, failure of the leader could be troublesome.

* Introduction

.image ./content/sensu_classic.svg _ 1000

Classic Sensu Architecture

* Introduction

.image ./content/sensu-go.svg _ 500

Sensu Go Architecture

* Introduction

- Sensu Go is built around etcd, and does not support RabbitMQ.
- I needed to come up with a model for round robin scheduling on etcd.
- I didn't want to follow the single leader pattern.
- I wanted round-robin scheduling to be as reliable as the store itself.
- Need to tolerate the loss of either scheduler or worker nodes.
- Would be nice to have a stable ordering of execution for round robin workers.

* Introduction

.image ./content/etcd-horizontal-color.svg _ 150

_A_distributed,_reliable_key-value_store_for_the_most_critical_data_of_a_distributed_system_ (etcd.io)

- Etcd is a distributed key-value database that uses Raft consensus.
- Written in Go. No Java or C components.
- Cross platform. Works on most Go compilation targets.
- Uses the BoltDB embedded database, optimized for SSDs.
- Uses gRPC as a transport, efficient RPC communication between peers and clients.

* Why etcd?

.image ./content/etcd-horizontal-color.svg _ 150

- Strongly consistent distributed key-value store.
- Benchmarked at tens of thousands of transactions per second.
- Can survive the loss of (n / 2) - 1 members.
- MVCC transaction model.
- Can be embedded in Go applications, no need for external dependency.
- Our goal was to enable a straightforward clustering story for Sensu Go.

* Raft

* Raft Consensus Algorithm

.image ./content/raft.png _ 500

* Raft Consensus Algorithm

- Created by Diego Ongaro and John Ousterhout at Stanford.
- Their goal was to replace Paxos (Leslie Lamport).
- Designed to maximize understandability.
- An algorithm for managing a replicated state machine and log.

* Raft Consensus Algorithm

- Why is it called raft?
- A raft is several logs tied together...
- A replicated log...

* Raft Consensus Algorithm

.image ./content/raft_deal_with_it.svg _ 500

* Raft Consensus Algorithm

- Raft is a consensus algorithm that is designed to maximize understandability.
- The algorithm manages a replicated state machine and log.
- Equivalent to Multi-Paxos, in power and efficiency.
- All algorithms of this class require a heartbeat, so raft has one too.
- Timeouts determine if a member is no longer alive.

* Raft

What is a log?

- A log is an append-only data structure.

What is a state machine?

- A state machine is a mathematical model for computation. It takes its input from a log.

How does this apply to raft?

- Each raft member has a state machine that consumes the replicated log. Because the log is guaranteed to be the same, the state machines will produce the same outputs.

* Raft

- Raft members are always in one of three states: leader, follower, candidate.
- Elections are used to determine the class of each member.
- If a follower has not seen a heartbeat for a long time, it establishes itself as a candidate and initiates an election.
- The result of the election process is that the follower will become the leader, or another cluster member will become the leader, or a timeout will occur.

* Raft

.image ./content/raft.svg _ 800

Replicated state machine architecture

* Raft

- Consensus algorithms guarantee safety (even with network delays, partitions, and packet loss, message duplication, and reordering).
- They are functional, AKA available, as long as a majority of their members are working and can communicate with one another.
- They do not depend on timing to ensure consistency in their logs. Bad clocks can cause availability problems at worst.

* Raft

- Raft has become more popular than Paxos, as it is easier to understand, and implement.
- Few people succeed in understanding Paxos, and it requires great effort to do so.
- Even seasoned researchers struggle at understanding Paxos.

.image ./content/noidea.jpg _ 500

* Raft

- Raft implements consensus by electing a leader, and making that leader responsible for managing the replicated log.
- The leader accepts log entries from clients, replicates them to followers, and tells them when it is safe to apply the logs to their state machines.
- Because the leader has the sole responsibility for managing the replicated log, it is free to append to the log in any way it likes.

* Raft

- Raft clusters are available as long as a majority of the members are working.
- All raft cluster sizes are odd numbers. (1, 3, 5, 7, 9)
- If 4 machines are members of a raft cluster, the cluster size is at least 5.

* Raft

- When more than (N/2 - 1) raft members fail, the cluster becomes unavailable.
- In a net split, the minority partition will not be available.
- This is essential to guarantee raft's correctness property.

* Raft

- The raft algorithm describes an infinitely growing log.
- Infinitely growing logs don't work so well in practice...

.image ./content/forever-ever.gif _ 600

* Raft

- Any useful implementation of raft requires some sort of log compaction.
- Many raft and paxos systems use snapshotting to deal with log compaction.
- Snapshotting can be implemented in various ways.
- After a snapshot, the log history to a certain point is compacted into a single entry.

.image ./content/compactor.gif _ 800

* Raft Key Takeaways

- Correctly implemented, a store built on raft will always be consistent.
- If more than half of a raft cluster fails, the service becomes unavailable.
- If a raft cluster is split in two, the smaller half becomes unavailable, while the larger half remains available, as long as it has a sufficient number of nodes.

* Back to etcd

* etcd API

What does the etcd API offer?

- Key-value storage (range, put, delete)
- Multi-version concurrency control
- Transactions (single round trip)
- Leases
- Watchers

* KV Storage

.image ./content/putgetdel.svg _ 800

* KV Storage

.image ./content/range.svg _ 800

* MVCC

.image ./content/mvcc.svg _ 800

* Transactions

- etcd's transactions are a single round-trip
- that means you can't read back a value, and then operate on it, transactionally
- but you can execute comparisons server-side

* Transactions

.play ./txn/txn.go /func main/,/^}/

* Leases

.image ./content/lease.svg _ 800

* Swiss Alps

.image ./content/suisse.jpg _ 700

* Leases (Keepalives)

.image ./content/keepalive.svg _ 800

* Leases (Keepalives)

- When combined with keepalives, leases offer a powerful primitive for creating etcd database triggers.
- In Sensu, leases are used for implementing vigilance control. When agents haven't been heard from for a long enough period, a leased key expires, which alerts the backend to the presence of failure.
- Leased keys can be combined with watchers for some interesting control flow constructs.

* Watchers

.image ./content/watchers.svg _ 800

* Leases and Watchers Together

.image ./content/epic_handshake.jpg _ 800

* Leases and Watchers Together

.image ./content/watch_leased.svg _ 800

* Mt. Baker

.image ./content/baker.jpg _ 750

* Lease and Watchers Together

- Allows creating a distributed trigger
- Semi-durable; can survive cluster downtime, but watch events are dropped if nobody is watching
- Forms the basis of a round-robin ring

* Round-robin ring

- A round-robin ring is a circular list of workers.
- The ring tracks which worker is the next to receive work.
- On a configurable interval, the workers travel around the ring, waiting for their turn to work.
- Unlike a token ring, the workers are not responsible for passing tokens to keep the ring mechanism working.

* Round-robin ring

- The round-robin ring is operated by one or more schedulers.
- Any scheduler can add or remove workers from the ring.
- The schedulers compete to advance the ring to the next position. (first write wins, lock-free)
- When a worker's turn to work comes up, every scheduler is notified. (watcher)

* Round-robin ring

- Workers in the ring are leased; if not kept alive, they will expire.
- This prevents the ring from containing workers that have failed. (eventually)
- Ring is lexicographically ordered, like etcd keys.
- The next worker to work is stored under a "next" key.
- If the schedulers notice that a worker has expired, and would have been the next to work, they compete to advance the ring.

* Round-robin ring

- Schedulers can fail, need at least one to keep scheduling working.
- If all schedulers fail, ring state is maintained.
- etcd servers can fail, ring will keep working as long as a majority are healthy.
- If etcd loses availability, ring state is maintained until restart.

* Round-robin ring

.image ./content/ring-seq-1.svg _ 600

* Round-robin ring

.image ./content/ring-seq-2.svg _ 600

* Round-robin ring

.image ./content/ring-seq-3.svg _ 600

* Round-robin ring

.image ./content/ring-seq-4.svg _ 600

* Round-robin ring

.image ./content/ring-seq-5.svg _ 600

* Round-robin ring

.image ./content/ring-seq-6.svg _ 600

* Round-robin ring

.image ./content/ring-seq-7.svg _ 600

* Round-robin ring

.image ./content/ring-seq-8.svg _ 600

* Testing

- Difficult to unit-test this library, eventually gave up.
- etcd interfaces are not easy to mock out.
- Created integration tests that run reasonably quickly.
- Easy to set up multiple etcd instances in a single Go process.

* Testing

- Early versions of the ring were not very successful!
- The first version lacked synchronization between the schedulers, and had unsolvable concurrency bugs.
- Moving the trigger mechanism into etcd, using leases, solved the synchronization problem.

* Testing

- etcd failures.
- Scheduler failures.
- Worker failures.
- Any combination of the above.

* What have I learned?

- The feature-set of etcd is quite interesting, and has some surprisingly powerful primitives.
- Testing complex data structures, and coordination routines, remains tricky. I am still learning how to best approach this.
- Lease expirations are remarkably un-performant. (Linear scan of all leases for every expiration, solved in etcd 3.4)

* "Why didn't you use a regular database?"

- It's complicated.
- I would always rather use Postgres. It's awesome, and has a richer model for transactions.
- Sometimes working within particular constraints can result in an interesting outcome.

* "Why didn't you use the single-leader pattern, but with etcd?"

- I was fearful of handling leader failure correctly.
- I believed that etcd provided the primitives for building a distributed, coordinated data structure.
- I perceived the single-leader pattern to be less reliable.
- It seemed more fun to do it this way.

* "Does it scale?"

- I think so... more testing needed is needed here.
- etcd watchers can scale surprisingly high with gRPC proxy - 1M watch events per second with 20 proxies.
- etcd should be able to service hundreds of scheduler nodes, maybe even thousands.
- A single scheduler should be able to handle thousands or tens of thousands of workers.

* Open-source implementation

Sensu's round-robin ring is available as a Go library under an MIT license.

.link https://godoc.org/github.com/sensu/sensu-go/backend/ringv2 https://godoc.org/github.com/sensu/sensu-go/backend/ringv2
